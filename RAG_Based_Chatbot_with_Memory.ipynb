{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fPJdp5a0QS6v"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --upgrade --quiet langchain langchain-community langchainhub langchain-chroma beautifulsoup4\n",
        "!pip install --q langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"LANGSMITH_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"rag-based-chatbot\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyARcgWkkSy9bgggJhYWcjWK85ZQP9BYAMQ\""
      ],
      "metadata": {
        "id": "pPvBGVjbRsTq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "mThdhBlhVgB-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ],
      "metadata": {
        "id": "RYjIABDrVmg9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-001\",convert_system_message_to_human=True)"
      ],
      "metadata": {
        "id": "IvYVCBgaV3BJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.invoke(\"Hi\").content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8eCqqtVjV3UU",
        "outputId": "4da556c3-7eaf-4eeb-885c-abeb5467d3c0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi! What can I do for you today? \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub"
      ],
      "metadata": {
        "id": "EI8M0lysW0ca"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain"
      ],
      "metadata": {
        "id": "14y744lqZSlP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain"
      ],
      "metadata": {
        "id": "jCGysb6bZVcM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma"
      ],
      "metadata": {
        "id": "Dk39DvRbZxrD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCmPrxoEaDJW",
        "outputId": "dda2bbef-90bf-4c4e-cb36-31a0831b8d93"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "FKVtLkeXaKkU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "dudepr5haQA5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import MessagesPlaceholder"
      ],
      "metadata": {
        "id": "w2qYkXyKaUSI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))),)"
      ],
      "metadata": {
        "id": "OnqY5Z2aaWTi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = loader.load()"
      ],
      "metadata": {
        "id": "Yg6OYM8jahqq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(doc)"
      ],
      "metadata": {
        "id": "NZxarvr5awNZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma.from_documents(documents=splits, embedding=gemini_embeddings)\n",
        "retriever = vectorstore.as_retriever()\n",
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGEvKe7Ua2me",
        "outputId": "9bbd62a3-ba88-4802-d392-5b6cd4b35fe1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['Chroma', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7ef9d4112e50>, search_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = (\n",
        "    \"You are an assistant designed to answer questions based on the provided context. \"\n",
        "    \"Rely only on the given information to respond. \"\n",
        "    \"If the answer isn't present in the context, simply say you don't know. \"\n",
        "    \"Keep your response brief and to the point, using no more than three sentences.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")"
      ],
      "metadata": {
        "id": "tOkjQzWha4E0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "OgFRkJFEbPWs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_answering_chain=create_stuff_documents_chain(model, chat_prompt)"
      ],
      "metadata": {
        "id": "wQYQhI2UbTQ-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = create_retrieval_chain(retriever, question_answering_chain)"
      ],
      "metadata": {
        "id": "RAihE2FdbhBb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke({\"input\":\"what is FAISS?\"})"
      ],
      "metadata": {
        "id": "RW0RjK6xbixT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TrmuujyrbmcI",
        "outputId": "af20f874-f843-44dd-f2aa-a58f8b713935"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'FAISS is a library for efficient similarity search and clustering of high-dimensional data. It applies vector quantization to partition the vector space into clusters, then refines quantization within each cluster. FAISS performs search by first looking for cluster candidates with coarse quantization and then further refining the search within each cluster. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import statistics\n",
        "\n",
        "def measure_baseline(query, runs=10):\n",
        "    times = []\n",
        "    for i in range(runs):\n",
        "        start_time = time.time()\n",
        "        # Invoke the chain with an empty chat_history list as required by the template.\n",
        "        _ = rag_chain.invoke({\"input\": query, \"chat_history\": []})\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"Run {i+1}: {elapsed_time:.3f} seconds\")\n",
        "        times.append(elapsed_time)\n",
        "    # Calculate and return the average time over all runs\n",
        "    return statistics.mean(times)\n",
        "\n",
        "# Specify the query for baseline measurement.\n",
        "baseline_query = \"what is FAISS?\"\n",
        "baseline_time = measure_baseline(baseline_query, runs=10)\n",
        "print(\"\\nBaseline time for query '{}': {:.3f} seconds\".format(baseline_query, baseline_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLyFd25uyrRr",
        "outputId": "87bf91a1-e6ab-4826-e7b2-932247e44b8e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 1: 1.033 seconds\n",
            "Run 2: 1.008 seconds\n",
            "Run 3: 1.107 seconds\n",
            "Run 4: 1.079 seconds\n",
            "Run 5: 0.830 seconds\n",
            "Run 6: 1.181 seconds\n",
            "Run 7: 1.024 seconds\n",
            "Run 8: 1.141 seconds\n",
            "Run 9: 1.575 seconds\n",
            "Run 10: 0.993 seconds\n",
            "\n",
            "Baseline time for query 'what is FAISS?': 1.097 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_history_aware_retriever"
      ],
      "metadata": {
        "id": "MjBJYalCbp26"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_prompt = (\n",
        "    \"Given a chat history and the user's latest question, which may refer to previous messages, \"\n",
        "    \"rephrase the question into a standalone version that makes sense without the prior context. \"\n",
        "    \"Do not answer the questionâ€”only reformulate it if necessary, or return it unchanged if it's already clear.\"\n",
        ")"
      ],
      "metadata": {
        "id": "CgorjfuobxEc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contextualize_q_prompt  = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", retriever_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "\n",
        "\n",
        "     ]\n",
        ")"
      ],
      "metadata": {
        "id": "f6hqK7HScAsZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_aware_retriever = create_history_aware_retriever(model,retriever,contextualize_q_prompt)"
      ],
      "metadata": {
        "id": "ueBJXNRtcDbn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain"
      ],
      "metadata": {
        "id": "bhn5cxfncJqJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain"
      ],
      "metadata": {
        "id": "pD3g1UPScLRi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "3MAY-mEecNUZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_answer_chain = create_stuff_documents_chain(model, qa_prompt)"
      ],
      "metadata": {
        "id": "oTZJRWKWdxtJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ],
      "metadata": {
        "id": "Ty3d-NX5dzks"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage"
      ],
      "metadata": {
        "id": "TlahuMY1d1Fl"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []"
      ],
      "metadata": {
        "id": "1t2L8YDEd3YS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question1 = \"what is Task Decomposition?\""
      ],
      "metadata": {
        "id": "ZYmxMY1bd41V"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message1= rag_chain.invoke({\"input\": question1, \"chat_history\": chat_history})"
      ],
      "metadata": {
        "id": "W6mNgqgLd6aY"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message1[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "6AaTARaCd8Zq",
        "outputId": "9adf83a3-7fc6-41d2-a4f7-c01eddfe9e2e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Task decomposition is a technique for breaking down complex tasks into smaller, simpler steps. It can be done by an LLM using simple prompting, task-specific instructions, or human input. The goal is to make tasks more manageable and provide insights into the model's thinking process. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history.extend(\n",
        "    [\n",
        "        HumanMessage(content=question1),\n",
        "        AIMessage(content=message1[\"answer\"]),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "RJCcmtLCd-wJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PhyZNlKeAr3",
        "outputId": "ff933694-89c1-4baa-f296-5b0a5987df9e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='what is Task Decomposition?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Task decomposition is a technique for breaking down complex tasks into smaller, simpler steps. It can be done by an LLM using simple prompting, task-specific instructions, or human input. The goal is to make tasks more manageable and provide insights into the model's thinking process. \", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_question = \"What are common ways of doing it?\"\n",
        "message2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
        "\n",
        "print(message2[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0jlB802eCEo",
        "outputId": "1d2b012d-8565-4344-e230-f03efa96d783"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The provided text describes three common ways of doing task decomposition:\n",
            "\n",
            "1. **LLM with simple prompting:**  Using prompts like \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\"\n",
            "2. **Task-specific instructions:** Providing instructions tailored to the task, such as \"Write a story outline.\" for writing a novel.\n",
            "3. **Human inputs:** Directly providing the task decomposition steps to the system. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "mHS7VZWleEPf"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store = {}"
      ],
      "metadata": {
        "id": "wQcGm0jdeH-3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]"
      ],
      "metadata": {
        "id": "d1TfFoKheJvB"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_rag_chain = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        "    output_messages_key=\"answer\",\n",
        ")"
      ],
      "metadata": {
        "id": "tG2iAelleLRF"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"What is Task Decomposition?\"},\n",
        "    config={\n",
        "        \"configurable\": {\"session_id\": \"abc123\"}\n",
        "    },  # constructs a key \"abc123\" in `store`.\n",
        ")[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "uXCRtZM5eMve",
        "outputId": "33bc2c06-e137-4c6f-98e2-bdf975dbd873"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be done using chain of thought (CoT) prompting, which instructs the model to \"think step by step\" to utilize more test-time computation.  Tree of Thoughts (ToT) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49y3W76QePzu",
        "outputId": "aec86876-265d-4554-9c5e-750525ce3964"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Task Decomposition?', additional_kwargs={}, response_metadata={}), AIMessage(content='Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be done using chain of thought (CoT) prompting, which instructs the model to \"think step by step\" to utilize more test-time computation.  Tree of Thoughts (ToT) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure. ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is Task Decomposition?', additional_kwargs={}, response_metadata={}), AIMessage(content='Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be done using chain of thought (CoT) prompting, which instructs the model to \"think step by step\" to utilize more test-time computation.  Tree of Thoughts (ToT) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure. ', additional_kwargs={}, response_metadata={})])}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"What are common ways of doing it?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
        ")[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Ge0Hp_fYeUgZ",
        "outputId": "a5596f0b-e313-4c5e-806f-cf53fa45a0a6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The provided text mentions three common ways of doing task decomposition:\\n\\n1. **LLM with simple prompting:**  Using prompts like \"Steps for XYZ.\\\\n1.\" or \"What are the subgoals for achieving XYZ?\" to guide the LLM.\\n2. **Task-specific instructions:**  Providing instructions tailored to the task, such as \"Write a story outline\" for writing a novel.\\n3. **Human inputs:**  Involving human input to decompose the task. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for message in store[\"abc123\"].messages:\n",
        "    if isinstance(message, AIMessage):\n",
        "        prefix = \"AI\"\n",
        "    else:\n",
        "        prefix = \"User\"\n",
        "\n",
        "    print(f\"{prefix}: {message.content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GryeZ_AReXLZ",
        "outputId": "9acf8443-d93f-4535-f5fe-1d6a06cbf464"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What is Task Decomposition?\n",
            "\n",
            "AI: Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be done using chain of thought (CoT) prompting, which instructs the model to \"think step by step\" to utilize more test-time computation.  Tree of Thoughts (ToT) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure. \n",
            "\n",
            "User: What is Task Decomposition?\n",
            "\n",
            "AI: Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be done using chain of thought (CoT) prompting, which instructs the model to \"think step by step\" to utilize more test-time computation.  Tree of Thoughts (ToT) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure. \n",
            "\n",
            "User: What are common ways of doing it?\n",
            "\n",
            "AI: The provided text mentions three common ways of doing task decomposition:\n",
            "\n",
            "1. **LLM with simple prompting:**  Using prompts like \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\" to guide the LLM.\n",
            "2. **Task-specific instructions:**  Providing instructions tailored to the task, such as \"Write a story outline\" for writing a novel.\n",
            "3. **Human inputs:**  Involving human input to decompose the task. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"What is a prompt technique like step xyz?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
        ")[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "70O4xMuXeY-9",
        "outputId": "35cf7968-9eb9-4c50-bc4a-01c6d32209e6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The prompt \"Steps for XYZ.\\\\n1.\" is an example of a simple prompting technique used for task decomposition. It instructs the LLM to generate a list of steps for completing the task represented by \"XYZ.\"  The \"1.\" at the end indicates that the LLM should start listing the steps from number one. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT5wuSPdee9p",
        "outputId": "870ef009-c9ce-441f-9178-bef2061fbef8"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Task Decomposition?', additional_kwargs={}, response_metadata={}), AIMessage(content='Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be done using chain of thought (CoT) prompting, which instructs the model to \"think step by step\" to utilize more test-time computation.  Tree of Thoughts (ToT) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure. ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is Task Decomposition?', additional_kwargs={}, response_metadata={}), AIMessage(content='Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be done using chain of thought (CoT) prompting, which instructs the model to \"think step by step\" to utilize more test-time computation.  Tree of Thoughts (ToT) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure. ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What are common ways of doing it?', additional_kwargs={}, response_metadata={}), AIMessage(content='The provided text mentions three common ways of doing task decomposition:\\n\\n1. **LLM with simple prompting:**  Using prompts like \"Steps for XYZ.\\\\n1.\" or \"What are the subgoals for achieving XYZ?\" to guide the LLM.\\n2. **Task-specific instructions:**  Providing instructions tailored to the task, such as \"Write a story outline\" for writing a novel.\\n3. **Human inputs:**  Involving human input to decompose the task. ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is a prompt technique like step xyz?', additional_kwargs={}, response_metadata={}), AIMessage(content='The prompt \"Steps for XYZ.\\\\n1.\" is an example of a simple prompting technique used for task decomposition. It instructs the LLM to generate a list of steps for completing the task represented by \"XYZ.\"  The \"1.\" at the end indicates that the LLM should start listing the steps from number one. ', additional_kwargs={}, response_metadata={})])}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for message in store[\"abc123\"].messages:\n",
        "    if isinstance(message, AIMessage):\n",
        "        prefix = \"AI\"\n",
        "    else:\n",
        "        prefix = \"User\"\n",
        "\n",
        "    print(f\"{prefix}: {message.content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqrbjRQdeho0",
        "outputId": "df1ef484-4a28-4292-cce6-808d925b0d34"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What is Task Decomposition?\n",
            "\n",
            "AI: Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be done using chain of thought (CoT) prompting, which instructs the model to \"think step by step\" to utilize more test-time computation.  Tree of Thoughts (ToT) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure. \n",
            "\n",
            "User: What is Task Decomposition?\n",
            "\n",
            "AI: Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be done using chain of thought (CoT) prompting, which instructs the model to \"think step by step\" to utilize more test-time computation.  Tree of Thoughts (ToT) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure. \n",
            "\n",
            "User: What are common ways of doing it?\n",
            "\n",
            "AI: The provided text mentions three common ways of doing task decomposition:\n",
            "\n",
            "1. **LLM with simple prompting:**  Using prompts like \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\" to guide the LLM.\n",
            "2. **Task-specific instructions:**  Providing instructions tailored to the task, such as \"Write a story outline\" for writing a novel.\n",
            "3. **Human inputs:**  Involving human input to decompose the task. \n",
            "\n",
            "User: What is a prompt technique like step xyz?\n",
            "\n",
            "AI: The prompt \"Steps for XYZ.\\n1.\" is an example of a simple prompting technique used for task decomposition. It instructs the LLM to generate a list of steps for completing the task represented by \"XYZ.\"  The \"1.\" at the end indicates that the LLM should start listing the steps from number one. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response Accuracy Improvement"
      ],
      "metadata": {
        "id": "JCPlM-lNuOlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# --- Current version measurement ---\n",
        "start_time = time.time()\n",
        "# Supply an empty chat_history as a list rather than a string.\n",
        "response = rag_chain.invoke({\"input\": \"what is FAISS?\", \"chat_history\": []})\n",
        "end_time = time.time()\n",
        "processing_time = end_time - start_time\n",
        "\n",
        "print(\"Current processing time for 'what is FAISS?': {:.3f} seconds\".format(processing_time))\n",
        "\n",
        "# Assuming a recorded baseline from an earlier version\n",
        "# baseline_time = 2.0  # seconds, replace with your actual baseline value\n",
        "time_reduction = ((baseline_time - processing_time) / baseline_time) * 100\n",
        "print(\"Processing time reduction: {:.2f}%\".format(time_reduction))"
      ],
      "metadata": {
        "id": "Z_k9HVRqiop9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc5e12e-b691-4bcb-cf6f-bc757378ed06"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current processing time for 'what is FAISS?': 0.853 seconds\n",
            "Processing time reduction: 22.26%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from statistics import mean\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Define a set of test queries.\n",
        "test_queries = [\n",
        "    \"what is FAISS?\",\n",
        "    \"what is Task Decomposition?\",\n",
        "    \"What are common ways of doing task decomposition?\",\n",
        "    \"What is a prompt technique like step xyz?\",\n",
        "]\n",
        "\n",
        "# Updated evaluation function:\n",
        "def evaluate_chain(chain, queries, chat_history=None, session_id=None):\n",
        "    response_times = []  # in seconds\n",
        "    answer_lengths = []  # number of words in the answer\n",
        "    successes = 0\n",
        "    total = len(queries)\n",
        "\n",
        "    # If no chat_history is provided, use an empty list.\n",
        "    if chat_history is None:\n",
        "        chat_history = []\n",
        "\n",
        "    for query in queries:\n",
        "        start_time = time.time()\n",
        "        # Prepare the payload.\n",
        "        # Some chain prompt templates expect \"chat_history\" even for basic chains.\n",
        "        payload = {\"input\": query, \"chat_history\": chat_history}\n",
        "\n",
        "        # Use session-based configuration if a session_id is provided.\n",
        "        if session_id:\n",
        "            response = chain.invoke(payload, config={\"configurable\": {\"session_id\": session_id}})\n",
        "        else:\n",
        "            response = chain.invoke(payload)\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        response_times.append(elapsed)\n",
        "\n",
        "        # Extract answer text and compute its word count.\n",
        "        answer_text = response.get(\"answer\", \"\").strip()\n",
        "        answer_word_count = len(answer_text.split())\n",
        "        answer_lengths.append(answer_word_count)\n",
        "\n",
        "        # Consider an answer successful if it contains text and isn't a default \"don't know\" answer.\n",
        "        if answer_text and \"don't know\" not in answer_text.lower():\n",
        "            successes += 1\n",
        "\n",
        "        # For conversational chains, update the history.\n",
        "        if session_id:\n",
        "            chat_history.extend([HumanMessage(content=query), AIMessage(content=answer_text)])\n",
        "\n",
        "        # Log details for each query.\n",
        "        print(f\"Query: '{query}'\\n-> Response: '{answer_text}'\\n-> Time: {elapsed:.2f} sec, Words: {answer_word_count}\\n\")\n",
        "\n",
        "    # Aggregate statistics.\n",
        "    stats = {\n",
        "        \"total_queries\": total,\n",
        "        \"avg_response_time_sec\": mean(response_times) if response_times else 0,\n",
        "        \"min_response_time_sec\": min(response_times) if response_times else 0,\n",
        "        \"max_response_time_sec\": max(response_times) if response_times else 0,\n",
        "        \"avg_word_count\": mean(answer_lengths) if answer_lengths else 0,\n",
        "        \"success_rate_percent\": (successes / total) * 100,\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "print(\"Evaluating Basic RAG Chain:\")\n",
        "basic_stats = evaluate_chain(rag_chain, test_queries)\n",
        "print(\"Basic Chain Metrics:\", basic_stats)\n",
        "\n",
        "print(\"\\nEvaluating Conversational RAG Chain with session_id 'abc123':\")\n",
        "# Initialize a fresh chat history for a conversational evaluation:\n",
        "chat_history = []\n",
        "conversational_stats = evaluate_chain(conversational_rag_chain, test_queries, chat_history, session_id=\"abc123\")\n",
        "print(\"Conversational Chain Metrics:\", conversational_stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Bou6uTD0fk4",
        "outputId": "4fb5d804-432f-40bc-8bfd-23212f9adc47"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Basic RAG Chain:\n",
            "Query: 'what is FAISS?'\n",
            "-> Response: 'FAISS is a library for efficient similarity search and clustering of high-dimensional data. It uses vector quantization to partition the vector space into clusters and then refines the quantization within clusters.  FAISS first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.'\n",
            "-> Time: 0.87 sec, Words: 50\n",
            "\n",
            "Query: 'what is Task Decomposition?'\n",
            "-> Response: 'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. It can be done by an LLM using prompting techniques like \"Steps for XYZ.\\n1.\", by using task-specific instructions, or with human input.'\n",
            "-> Time: 0.83 sec, Words: 38\n",
            "\n",
            "Query: 'What are common ways of doing task decomposition?'\n",
            "-> Response: 'Task decomposition can be done in three ways: (1) by using simple prompts, (2) by using task-specific instructions, or (3) with human inputs.'\n",
            "-> Time: 0.80 sec, Words: 23\n",
            "\n",
            "Query: 'What is a prompt technique like step xyz?'\n",
            "-> Response: 'I'm sorry, but the provided text does not contain information about a prompt technique called \"step xyz.\" Therefore, I cannot answer your question.'\n",
            "-> Time: 0.78 sec, Words: 23\n",
            "\n",
            "Basic Chain Metrics: {'total_queries': 4, 'avg_response_time_sec': 0.8204950094223022, 'min_response_time_sec': 0.784686803817749, 'max_response_time_sec': 0.8711202144622803, 'avg_word_count': 33.5, 'success_rate_percent': 100.0}\n",
            "\n",
            "Evaluating Conversational RAG Chain with session_id 'abc123':\n",
            "Query: 'what is FAISS?'\n",
            "-> Response: 'FAISS (Facebook AI Similarity Search) is a library for efficient similarity search in high-dimensional spaces. It works by applying vector quantization, partitioning the vector space into clusters and refining quantization within those clusters. This allows FAISS to quickly find similar data points by first searching for candidate clusters and then refining the search within those clusters.'\n",
            "-> Time: 3.15 sec, Words: 56\n",
            "\n",
            "Query: 'what is Task Decomposition?'\n",
            "-> Response: 'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be done using chain of thought (CoT) prompting, which instructs the model to \"think step by step\" to utilize more test-time computation.  Tree of Thoughts (ToT) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure.'\n",
            "-> Time: 2.02 sec, Words: 58\n",
            "\n",
            "Query: 'What are common ways of doing task decomposition?'\n",
            "-> Response: 'The provided text mentions three common ways of doing task decomposition:\n",
            "\n",
            "1. **LLM with simple prompting:**  Using prompts like \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\" to guide the LLM.\n",
            "2. **Task-specific instructions:**  Providing instructions tailored to the task, such as \"Write a story outline\" for writing a novel.\n",
            "3. **Human inputs:**  Involving human input to decompose the task.'\n",
            "-> Time: 3.07 sec, Words: 63\n",
            "\n",
            "Query: 'What is a prompt technique like step xyz?'\n",
            "-> Response: 'The prompt \"Steps for XYZ.\\n1.\" is an example of a simple prompting technique used for task decomposition. It instructs the LLM to generate a list of steps for completing the task represented by \"XYZ.\"  The \"1.\" at the end indicates that the LLM should start listing the steps from number one.'\n",
            "-> Time: 1.48 sec, Words: 51\n",
            "\n",
            "Conversational Chain Metrics: {'total_queries': 4, 'avg_response_time_sec': 2.4314481019973755, 'min_response_time_sec': 1.4806902408599854, 'max_response_time_sec': 3.150800943374634, 'avg_word_count': 57, 'success_rate_percent': 100.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zZkNxuRP04dC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}